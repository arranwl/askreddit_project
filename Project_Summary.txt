Project Summary

The motivation from this project stems from multiple places. The first of which was that I wanted to work with language data. Natural Language Processing (NLP) is a field that is becoming increasingly interesting to me, and also one I’ve found little connection to in my regular course work. The second one was that I wanted to use self collected data. Many data science projects start with a cleaned and prepped data set, which is not representative of real life, and so I wanted to make sure the data I used would be scrapped by myself. The third and final motivation for this project was using some form of a neural network. I had yet to work with one outside of class, and learning about them this semester made me excited to try using one.


The goal of this project was to predict the score on posts made in the subreddit “AskReddit”, a open forum where users post discussion questions and other users can answer. I had planned to try predicting  a specific score. This seemed like the obvious option for this project, however I quickly found it difficult to get predictions within an acceptable range. It was either vastly over predicted or under predicted. Most of my experimental models in this style did not do a good job either way. I realized this is because the data was not unbalanced in that most scores were under 100, or the few that “made it” so to say had a score well over 1000. From this I decided it could be easier on the model to predict a category that corresponds to a range of scores that the post could fall into. Practically speaking, this makes sense as a user since a post that gets a score more than 1000 will show up on the “Hot” section of the page, which most users read.


This change was followed by another interesting quirk. I had initially filtered out stop words from the word frequency table. Stop words are words that add no value to the text, like “and”, “or”, “like”, or “what”. However, I realized that my stop words were not getting properly filtered out if they had an upper case character. When I fixed my error and retrained the model, both with the flags and times, and without, the model could not get above a 56% accuracy on the training set. This was unforeseen, as while I was getting in the low 60% accuracy on the test set, I had been getting 98% or above on the training set. This seemed absurd, but low and behold when I stopped filtering out stop words completely, I achieved the highest accuracy on the test set yet: 65%. 


This really confounded me, and so I have a theory as to why including stop words changes the model performance so much. It really stems from the idea that the popular questions that gain traction are never complex questions that contain many non-stop words of depth. The questions that are popular are ones like “What are you feelings about x”, “People who have had y happen to you, what is your story”, or “Z category of Redditors, what is your opinion on xyz”. There often lacks many words that provide detail or insight, and so by removing stop words a lot of actual data in a question is lost. On a large scale, people don’t tend to engage with questions that are full of nuanced content on a website primarily used for procrastination and entertainment.


The low model performance was slightly disappointing. I had hoped to achieve a testing accuracy of at least 80%, and I really tried to squeeze what I could out of the data. However, I think the value from this project was less so about the end result, and more so about the actual process. I got to work with a new package, work with NLP, design my own Multi Layer Perceptron Neural Network, and deploy fun web app. While I am satisfied with the project, I think this is an idea I’d like to approach again given some more tools, as well as more and different data.